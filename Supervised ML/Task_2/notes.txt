Overfitting:
    Overfitting occurs when a model learns the training data too well, capturing noise and details that do not generalize to new data.
    This results in poor performance on unseen data.

Underfitting:
    Underfitting happens when a model is too simple to capture the underlying patterns in the data, leading to poor performance on both training and test sets.

Balanced Fit:
    A model that generalizes well to both training and test data, capturing the underlying patterns without overcomplicating the model.

Regularization:
    Regularization techniques are used to prevent overfitting by adding a penalty to the model's loss function.
    The goal is to shrink the model's parameters (weights) to reduce complexity.

Types of Regularization:

L1 Regularization (LASSO Regression):
    Adds the absolute value of the model's coefficients (weights) to the loss function.
    Encourages sparsity in the model by driving some coefficients to zero, effectively performing feature selection.

L2 Regularization (Ridge Regression):
   Adds the squared value of the model's coefficients to the loss function.
   Shrinks all coefficients but does not force them to zero, maintaining all features.

Key Differences:

L1:
    Produces sparse models (some coefficients become zero).
    Useful for feature selection.

L2:
    Shrinks coefficients but keeps all features.
    Prevents overfitting without eliminating features.

